<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Evaluation · Recommendation.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Recommendation.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Recommendation.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../data/">Preparing Data</a></li><li><a class="tocitem" href="../notation/">Notation</a></li><li><a class="tocitem" href="../baseline/">Non-Personalized Baselines</a></li><li><a class="tocitem" href="../collaborative_filtering/">Collaborative Filtering</a></li><li><a class="tocitem" href="../factorization_machines/">Factorization Machines</a></li><li><a class="tocitem" href="../content_based_filtering/">Content-Based Filtering</a></li><li class="is-active"><a class="tocitem" href>Evaluation</a><ul class="internal"><li><a class="tocitem" href="#Cross-validation"><span>Cross validation</span></a></li><li><a class="tocitem" href="#Rating-metrics"><span>Rating metrics</span></a></li><li><a class="tocitem" href="#Ranking-metrics"><span>Ranking metrics</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Evaluation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Evaluation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/takuti/Recommendation.jl/blob/master/docs/src/evaluation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h1><ul><li><a href="#Recommendation.AUC"><code>Recommendation.AUC</code></a></li><li><a href="#Recommendation.MAE"><code>Recommendation.MAE</code></a></li><li><a href="#Recommendation.MAP"><code>Recommendation.MAP</code></a></li><li><a href="#Recommendation.MPR"><code>Recommendation.MPR</code></a></li><li><a href="#Recommendation.NDCG"><code>Recommendation.NDCG</code></a></li><li><a href="#Recommendation.Precision"><code>Recommendation.Precision</code></a></li><li><a href="#Recommendation.RMSE"><code>Recommendation.RMSE</code></a></li><li><a href="#Recommendation.Recall"><code>Recommendation.Recall</code></a></li><li><a href="#Recommendation.ReciprocalRank"><code>Recommendation.ReciprocalRank</code></a></li><li><a href="#Recommendation.cross_validation"><code>Recommendation.cross_validation</code></a></li></ul><h2 id="Cross-validation"><a class="docs-heading-anchor" href="#Cross-validation">Cross validation</a><a id="Cross-validation-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-validation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Recommendation.cross_validation" href="#Recommendation.cross_validation"><code>Recommendation.cross_validation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">cross_validation(
    n_folds::Integer,
    metric::Type{&lt;:RankingMetric},
    k::Integer,
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct <code>n_folds</code> cross validation for a combination of recommender <code>recommender_type</code> and ranking metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code> and runs top-<code>k</code> recommendation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/evaluation/cross_validation.jl#L3-L14">source</a></section><section><div><pre><code class="language-none">cross_validation(
    n_folds::Integer,
    metric::Type{&lt;:AccuracyMetric},
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct <code>n_folds</code> cross validation for a combination of recommender <code>recommender_type</code> and accuracy metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/evaluation/cross_validation.jl#L46-L56">source</a></section></article><h2 id="Rating-metrics"><a class="docs-heading-anchor" href="#Rating-metrics">Rating metrics</a><a id="Rating-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Rating-metrics" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Recommendation.RMSE" href="#Recommendation.RMSE"><code>Recommendation.RMSE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RMSE</code></pre><p>Root Mean Squared Error.</p><pre><code class="language-none">measure(
    metric::RMSE,
    truth::AbstractVector,
    pred::AbstractVector
)</code></pre><p><code>truth</code> and <code>pred</code> are expected to be the same size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/accuracy.jl#L3-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.MAE" href="#Recommendation.MAE"><code>Recommendation.MAE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MAE</code></pre><p>Mean Absolute Error.</p><pre><code class="language-none">measure(
    metric::MAE,
    truth::AbstractVector,
    pred::AbstractVector
)</code></pre><p><code>truth</code> and <code>pred</code> are expected to be the same size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/accuracy.jl#L29-L41">source</a></section></article><h2 id="Ranking-metrics"><a class="docs-heading-anchor" href="#Ranking-metrics">Ranking metrics</a><a id="Ranking-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Ranking-metrics" title="Permalink"></a></h2><p>Let a target user <span>$u \in \mathcal{U}$</span>, set of all items <span>$\mathcal{I}$</span>, ordered set of top-<span>$N$</span> recommended items <span>$I_N(u) \subset \mathcal{I}$</span>, and set of truth items <span>$\mathcal{I}^+_u$</span>.</p><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Recall" href="#Recommendation.Recall"><code>Recommendation.Recall</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Recall</code></pre><p>Recall-at-<span>$N$</span> (Recall@<span>$N$</span>) indicates coverage of truth samples as a result of top-<span>$N$</span> recommendation. The value is computed by the following equation:</p><p class="math-container">\[\mathrm{Recall@}N = \frac{|\mathcal{I}^+_u \cap I_N(u)|}{|\mathcal{I}^+_u|}.\]</p><p>Here, <span>$|\mathcal{I}^+_u \cap I_N(u)|$</span> is the number of <em>true positives</em>.</p><pre><code class="language-none">measure(
    metric::Recall,
    truth::AbstractVector{T},
    pred::AbstractVector{T},
    k::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Precision" href="#Recommendation.Precision"><code>Recommendation.Precision</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Precision</code></pre><p>Precision-at-<span>$N$</span> (Precision@<span>$N$</span>) evaluates correctness of a top-<span>$N$</span> recommendation list <span>$I_N(u)$</span> according to the portion of true positives in the list as:</p><p class="math-container">\[\mathrm{Precision@}N = \frac{|\mathcal{I}^+_u \cap I_N(u)|}{|I_N(u)|}.\]</p><pre><code class="language-none">measure(
    metric::Precision,
    truth::AbstractVector{T},
    pred::AbstractVector{T},
    k::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L24-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.MAP" href="#Recommendation.MAP"><code>Recommendation.MAP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MAP</code></pre><p>While the original Precision@<span>$N$</span> provides a score for a fixed-length recommendation list <span>$I_N(u)$</span>, mean average precision (MAP) computes an average of the scores over all recommendation sizes from 1 to <span>$|\mathcal{I}|$</span>. MAP is formulated with an indicator function for <span>$i_n$</span>, the <span>$n$</span>-th item of <span>$I(u)$</span>, as:</p><p class="math-container">\[\mathrm{MAP} = \frac{1}{|\mathcal{I}^+_u|} \sum_{n = 1}^{|\mathcal{I}|} \mathrm{Precision@}n \cdot  \mathbb{1}_{\mathcal{I}^+_u}(i_n).\]</p><p>It should be noticed that, MAP is not a simple mean of sum of Precision@<span>$1$</span>, Precision@<span>$2$</span>, <span>$\dots$</span>, Precision@<span>$|\mathcal{I}|$</span>, and higher-ranked true positives lead better MAP.</p><pre><code class="language-none">measure(
    metric::MAP,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L44-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.AUC" href="#Recommendation.AUC"><code>Recommendation.AUC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AUC</code></pre><p>ROC curve and area under the ROC curve (AUC) are generally used in evaluation of the classification problems, but these concepts can also be interpreted in a context of ranking problem. Basically, the AUC metric for ranking considers all possible pairs of truth and other items which are respectively denoted by <span>$i^+ \in \mathcal{I}^+_u$</span> and <span>$i^- \in \mathcal{I}^-_u$</span>, and it expects that the <span>$best&#39;&#39; recommender completely ranks$</span>i^+<span>$higher than$</span>i^-``, as follows:</p><p><img src="../assets/images/auc.png" alt="auc"/></p><p>AUC calculation keeps track the number of true positives at different rank in <span>$\mathcal{I}$</span>. At line 8, the function adds the number of true positives which were ranked higher than the current non-truth sample to the accumulated count of correct pairs. Ultimately, an AUC score is computed as portion of the correct ordered <span>$(i^+, i^-)$</span> pairs in the all possible combinations determined by <span>$|\mathcal{I}^+_u| \times |\mathcal{I}^-_u|$</span> in set notation.</p><pre><code class="language-none">measure(
    metric::AUC,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L75-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.ReciprocalRank" href="#Recommendation.ReciprocalRank"><code>Recommendation.ReciprocalRank</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ReciprocalRank</code></pre><p>If we are only interested in the first true positive, reciprocal rank (RR) could be a reasonable choice to quantitatively assess the recommendation lists. For <span>$n_{\mathrm{tp}} \in \left[ 1, |\mathcal{I}| \right]$</span>, a position of the first true positive in <span>$I(u)$</span>, RR simply returns its inverse:</p><p class="math-container">\[  \mathrm{RR} = \frac{1}{n_{\mathrm{tp}}}.\]</p><p>RR can be zero if and only if <span>$\mathcal{I}^+_u$</span> is empty.</p><pre><code class="language-none">measure(
    metric::ReciprocalRank,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L106-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.MPR" href="#Recommendation.MPR"><code>Recommendation.MPR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MPR</code></pre><p>Mean percentile rank (MPR) is a ranking metric based on <span>$r_{i} \in [0, 100]$</span>, the percentile-ranking of an item <span>$i$</span> within the sorted list of all items for a user <span>$u$</span>. It can be formulated as:</p><p class="math-container">\[\mathrm{MPR} = \frac{1}{|\mathcal{I}^+_u|} \sum_{i \in \mathcal{I}^+_u} r_{i}.\]</p><p><span>$r_{i} = 0\%$</span> is the best value that means the truth item <span>$i$</span> is ranked at the highest position in a recommendation list. On the other hand, <span>$r_{i} = 100\%$</span> is the worst case that the item <span>$i$</span> is at the lowest rank.</p><p>MPR internally considers not only top-<span>$N$</span> recommended items also all of the non-recommended items, and it accumulates the percentile ranks for all true positives unlike MRR. So, the measure is suitable to estimate users&#39; overall satisfaction for a recommender. Intuitively, <span>$\mathrm{MPR} &gt; 50\%$</span> should be worse than random ranking from a users&#39; point of view.</p><pre><code class="language-none">measure(
    metric::MPR,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L133-L149">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.NDCG" href="#Recommendation.NDCG"><code>Recommendation.NDCG</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NDCG</code></pre><p>Like MPR, normalized discounted cumulative gain (NDCG) computes a score for <span>$I(u)$</span> which places emphasis on higher-ranked true positives. In addition to being a more well-formulated measure, the difference between NDCG and MPR is that NDCG allows us to specify an expected ranking within <span>$\mathcal{I}^+_u$</span>; that is, the metric can incorporate <span>$\mathrm{rel}_n$</span>, a relevance score which suggests how likely the <span>$n$</span>-th sample is to be ranked at the top of a recommendation list, and it directly corresponds to an expected ranking of the truth samples.</p><pre><code class="language-none">measure(
    metric::NDCG,
    truth::AbstractVector{T},
    pred::AbstractVector{T},
    k::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/3c9484b9b08b8d51f6a4f6e4ad4c843b9d3ecc4e/src/metrics/ranking.jl#L161-L172">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../content_based_filtering/">« Content-Based Filtering</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 3 March 2022 13:41">Thursday 3 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
