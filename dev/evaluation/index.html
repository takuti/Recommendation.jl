<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Evaluation · Recommendation.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="Recommendation.jl logo"/></a><h1>Recommendation.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../getting_started/">Getting Started</a></li><li><span class="toctext">References</span><ul><li><a class="toctext" href="../notation/">Notation</a></li><li><a class="toctext" href="../baseline/">Non-Personalized Baselines</a></li><li><a class="toctext" href="../collaborative_filtering/">Collaborative Filtering</a></li><li><a class="toctext" href="../content_based_filtering/">Content-Based Filtering</a></li><li class="current"><a class="toctext" href>Evaluation</a><ul class="internal"><li><a class="toctext" href="#Cross-validation-1">Cross validation</a></li><li><a class="toctext" href="#Rating-metric-1">Rating metric</a></li><li><a class="toctext" href="#Ranking-metric-1">Ranking metric</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>References</li><li><a href>Evaluation</a></li></ul><a class="edit-page" href="https://github.com/takuti/Recommendation.jl/blob/master/docs/src/evaluation.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Evaluation</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Evaluation-1" href="#Evaluation-1">Evaluation</a></h1><ul><li><a href="#Recommendation.AUC"><code>Recommendation.AUC</code></a></li><li><a href="#Recommendation.MAE"><code>Recommendation.MAE</code></a></li><li><a href="#Recommendation.MAP"><code>Recommendation.MAP</code></a></li><li><a href="#Recommendation.MPR"><code>Recommendation.MPR</code></a></li><li><a href="#Recommendation.NDCG"><code>Recommendation.NDCG</code></a></li><li><a href="#Recommendation.Precision"><code>Recommendation.Precision</code></a></li><li><a href="#Recommendation.RMSE"><code>Recommendation.RMSE</code></a></li><li><a href="#Recommendation.Recall"><code>Recommendation.Recall</code></a></li><li><a href="#Recommendation.ReciprocalRank"><code>Recommendation.ReciprocalRank</code></a></li><li><a href="#Recommendation.cross_validation"><code>Recommendation.cross_validation</code></a></li></ul><h2><a class="nav-anchor" id="Cross-validation-1" href="#Cross-validation-1">Cross validation</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.cross_validation" href="#Recommendation.cross_validation"><code>Recommendation.cross_validation</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cross_validation(
    n_fold::Int,
    metric::Type{&lt;:Metric},
    k::Int,
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct <code>n_fold</code> cross validation for a combination of recommender <code>recommender_type</code> and metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code>. For ranking metric, accuracy is measured by top-<code>k</code> recommendation.</p></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/evaluation/cross_validation.jl#L3-L14">source</a></section><h2><a class="nav-anchor" id="Rating-metric-1" href="#Rating-metric-1">Rating metric</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.RMSE" href="#Recommendation.RMSE"><code>Recommendation.RMSE</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">RMSE</code></pre><p>Root Mean Squared Error.</p><pre><code class="language-none">measure(
    metric::RMSE,
    truth::AbstractVector,
    pred::AbstractVector
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/accuracy.jl#L3-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.MAE" href="#Recommendation.MAE"><code>Recommendation.MAE</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MAE</code></pre><p>Mean Absolute Error.</p><pre><code class="language-none">measure(
    metric::MAE,
    truth::AbstractVector,
    pred::AbstractVector
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/accuracy.jl#L19-L29">source</a></section><h2><a class="nav-anchor" id="Ranking-metric-1" href="#Ranking-metric-1">Ranking metric</a></h2><p>Let a target user <span>$u \in \mathcal{U}$</span>, set of all items <span>$\mathcal{I}$</span>, ordered set of top-<span>$N$</span> recommended items <span>$I_N(u) \subset \mathcal{I}$</span>, and set of truth items <span>$\mathcal{I}^+_u$</span>.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.Recall" href="#Recommendation.Recall"><code>Recommendation.Recall</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Recall</code></pre><p>Recall-at-<span>$N$</span> (Recall@<span>$N$</span>) indicates coverage of truth samples as a result of top-<span>$N$</span> recommendation. The value is computed by the following equation:</p><div>\[\mathrm{Recall@}N = \frac{|\mathcal{I}^+_u \cap I_N(u)|}{|\mathcal{I}^+_u|}.\]</div><p>Here, <span>$|\mathcal{I}^+_u \cap I_N(u)|$</span> is the number of <em>true positives</em>.</p><pre><code class="language-none">measure(
    metric::Recall,
    truth::Array{T},
    pred::Array{T},
    k::Int
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L3-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.Precision" href="#Recommendation.Precision"><code>Recommendation.Precision</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Precision</code></pre><p>Precision-at-<span>$N$</span> (Precision@<span>$N$</span>) evaluates correctness of a top-<span>$N$</span> recommendation list <span>$I_N(u)$</span> according to the portion of true positives in the list as:</p><div>\[\mathrm{Precision@}N = \frac{|\mathcal{I}^+_u \cap I_N(u)|}{|I_N(u)|}.\]</div><pre><code class="language-none">measure(
    metric::Precision,
    truth::Array{T},
    pred::Array{T},
    k::Int
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L24-L38">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.MAP" href="#Recommendation.MAP"><code>Recommendation.MAP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MAP</code></pre><p>While the original Precision@<span>$N$</span> provides a score for a fixed-length recommendation list <span>$I_N(u)$</span>, mean average precision (MAP) computes an average of the scores over all recommendation sizes from 1 to <span>$|\mathcal{I}|$</span>. MAP is formulated with an indicator function for <span>$i_n$</span>, the <span>$n$</span>-th item of <span>$I(u)$</span>, as:</p><div>\[\mathrm{MAP} = \frac{1}{|\mathcal{I}^+_u|} \sum_{n = 1}^{|\mathcal{I}|} \mathrm{Precision@}n \cdot  \mathbb{1}_{\mathcal{I}^+_u}(i_n).\]</div><p>It should be noticed that, MAP is not a simple mean of sum of Precision@<span>$1$</span>, Precision@<span>$2$</span>, <span>$\dots$</span>, Precision@<span>$|\mathcal{I}|$</span>, and higher-ranked true positives lead better MAP.</p><pre><code class="language-none">measure(
    metric::MAP,
    truth::Array{T},
    pred::Array{T}
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L44-L59">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.AUC" href="#Recommendation.AUC"><code>Recommendation.AUC</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">AUC</code></pre><p>ROC curve and area under the ROC curve (AUC) are generally used in evaluation of the classification problems, but these concepts can also be interpreted in a context of ranking problem. Basically, the AUC metric for ranking considers all possible pairs of truth and other items which are respectively denoted by <span>$i^+ \in \mathcal{I}^+_u$</span> and <span>$i^- \in \mathcal{I}^-_u$</span>, and it expects that the <span>$best&#39;&#39; recommender completely ranks$</span>i^+<span>$higher than$</span>i^-``, as follows:</p><p><img src="../assets/images/auc.png" alt="auc"/></p><p>AUC calculation keeps track the number of true positives at different rank in <span>$\mathcal{I}$</span>. At line 8, the function adds the number of true positives which were ranked higher than the current non-truth sample to the accumulated count of correct pairs. Ultimately, an AUC score is computed as portion of the correct ordered <span>$(i^+, i^-)$</span> pairs in the all possible combinations determined by <span>$|\mathcal{I}^+_u| \times |\mathcal{I}^-_u|$</span> in set notation.</p><pre><code class="language-none">measure(
    metric::AUC,
    truth::Array{T},
    pred::Array{T}
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L75-L89">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.ReciprocalRank" href="#Recommendation.ReciprocalRank"><code>Recommendation.ReciprocalRank</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ReciprocalRank</code></pre><p>If we are only interested in the first true positive, reciprocal rank (RR) could be a reasonable choice to quantitatively assess the recommendation lists. For <span>$n_{\mathrm{tp}} \in \left[ 1, |\mathcal{I}| \right]$</span>, a position of the first true positive in <span>$I(u)$</span>, RR simply returns its inverse:</p><div>\[  \mathrm{RR} = \frac{1}{n_{\mathrm{tp}}}.\]</div><p>RR can be zero if and only if <span>$\mathcal{I}^+_u$</span> is empty.</p><pre><code class="language-none">measure(
    metric::ReciprocalRank,
    truth::Array{T},
    pred::Array{T}
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L106-L121">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.MPR" href="#Recommendation.MPR"><code>Recommendation.MPR</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MPR</code></pre><p>Mean percentile rank (MPR) is a ranking metric based on <span>$r_{i} \in [0, 100]$</span>, the percentile-ranking of an item <span>$i$</span> within the sorted list of all items for a user <span>$u$</span>. It can be formulated as:</p><div>\[\mathrm{MPR} = \frac{1}{|\mathcal{I}^+_u|} \sum_{i \in \mathcal{I}^+_u} r_{i}.\]</div><p><span>$r_{i} = 0\%$</span> is the best value that means the truth item <span>$i$</span> is ranked at the highest position in a recommendation list. On the other hand, <span>$r_{i} = 100\%$</span> is the worst case that the item <span>$i$</span> is at the lowest rank.</p><p>MPR internally considers not only top-<span>$N$</span> recommended items also all of the non-recommended items, and it accumulates the percentile ranks for all true positives unlike MRR. So, the measure is suitable to estimate users&#39; overall satisfaction for a recommender. Intuitively, <span>$\mathrm{MPR} &gt; 50\%$</span> should be worse than random ranking from a users&#39; point of view.</p><pre><code class="language-none">measure(
    metric::MPR,
    truth::Array{T},
    pred::Array{T}
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L133-L149">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Recommendation.NDCG" href="#Recommendation.NDCG"><code>Recommendation.NDCG</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">NDCG</code></pre><p>Like MPR, normalized discounted cumulative gain (NDCG) computes a score for <span>$I(u)$</span> which places emphasis on higher-ranked true positives. In addition to being a more well-formulated measure, the difference between NDCG and MPR is that NDCG allows us to specify an expected ranking within <span>$\mathcal{I}^+_u$</span>; that is, the metric can incorporate <span>$\mathrm{rel}_n$</span>, a relevance score which suggests how likely the <span>$n$</span>-th sample is to be ranked at the top of a recommendation list, and it directly corresponds to an expected ranking of the truth samples.</p><pre><code class="language-none">measure(
    metric::NDCG,
    truth::Array{T},
    pred::Array{T},
    k::Int
)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/1e156daa901bf3319c4f442a61970796ce4c4574/src/metric/ranking.jl#L161-L172">source</a></section><footer><hr/><a class="previous" href="../content_based_filtering/"><span class="direction">Previous</span><span class="title">Content-Based Filtering</span></a></footer></article></body></html>
