<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Evaluation · Recommendation.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Recommendation.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Recommendation.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../data/">Preparing Data</a></li><li><a class="tocitem" href="../notation/">Notation</a></li><li><a class="tocitem" href="../baseline/">Non-Personalized Baselines</a></li><li><a class="tocitem" href="../collaborative_filtering/">Collaborative Filtering</a></li><li><a class="tocitem" href="../factorization_machines/">Factorization Machines</a></li><li><a class="tocitem" href="../content_based_filtering/">Content-Based Filtering</a></li><li class="is-active"><a class="tocitem" href>Evaluation</a><ul class="internal"><li><a class="tocitem" href="#Cross-validation"><span>Cross validation</span></a></li><li><a class="tocitem" href="#Rating-metrics"><span>Rating metrics</span></a></li><li><a class="tocitem" href="#Ranking-metrics"><span>Ranking metrics</span></a></li><li><a class="tocitem" href="#Aggregated-metrics"><span>Aggregated metrics</span></a></li><li><a class="tocitem" href="#Intra-list-metrics"><span>Intra-list metrics</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Evaluation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Evaluation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/takuti/Recommendation.jl/blob/master/docs/src/evaluation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h1><ul><li><a href="#Recommendation.AUC"><code>Recommendation.AUC</code></a></li><li><a href="#Recommendation.AggregatedDiversity"><code>Recommendation.AggregatedDiversity</code></a></li><li><a href="#Recommendation.Coverage"><code>Recommendation.Coverage</code></a></li><li><a href="#Recommendation.GiniIndex"><code>Recommendation.GiniIndex</code></a></li><li><a href="#Recommendation.IntraListSimilarity"><code>Recommendation.IntraListSimilarity</code></a></li><li><a href="#Recommendation.MAE"><code>Recommendation.MAE</code></a></li><li><a href="#Recommendation.MAP"><code>Recommendation.MAP</code></a></li><li><a href="#Recommendation.MPR"><code>Recommendation.MPR</code></a></li><li><a href="#Recommendation.NDCG"><code>Recommendation.NDCG</code></a></li><li><a href="#Recommendation.Novelty"><code>Recommendation.Novelty</code></a></li><li><a href="#Recommendation.Precision"><code>Recommendation.Precision</code></a></li><li><a href="#Recommendation.RMSE"><code>Recommendation.RMSE</code></a></li><li><a href="#Recommendation.Recall"><code>Recommendation.Recall</code></a></li><li><a href="#Recommendation.ReciprocalRank"><code>Recommendation.ReciprocalRank</code></a></li><li><a href="#Recommendation.Serendipity"><code>Recommendation.Serendipity</code></a></li><li><a href="#Recommendation.ShannonEntropy"><code>Recommendation.ShannonEntropy</code></a></li><li><a href="#Recommendation.cross_validation"><code>Recommendation.cross_validation</code></a></li><li><a href="#Recommendation.leave_one_out"><code>Recommendation.leave_one_out</code></a></li></ul><h2 id="Cross-validation"><a class="docs-heading-anchor" href="#Cross-validation">Cross validation</a><a id="Cross-validation-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-validation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Recommendation.cross_validation" href="#Recommendation.cross_validation"><code>Recommendation.cross_validation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">cross_validation(
    n_folds::Integer,
    metric::Type{&lt;:RankingMetric},
    topk::Integer,
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct <code>n_folds</code> cross validation for a combination of recommender <code>recommender_type</code> and ranking metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code> and runs top-<code>k</code> recommendation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/evaluation/cross_validation.jl#L3-L14">source</a></section><section><div><pre><code class="language-none">cross_validation(
    n_folds::Integer,
    metric::Type{&lt;:AccuracyMetric},
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct <code>n_folds</code> cross validation for a combination of recommender <code>recommender_type</code> and accuracy metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/evaluation/cross_validation.jl#L25-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.leave_one_out" href="#Recommendation.leave_one_out"><code>Recommendation.leave_one_out</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">leave_one_out(
    metric::Type{&lt;:RankingMetric},
    topk::Integer,
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct leave-one-out cross validation (LOOCV) for a combination of recommender <code>recommender_type</code> and accuracy metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code> and runs top-<code>k</code> recommendation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/evaluation/cross_validation.jl#L46-L56">source</a></section><section><div><pre><code class="language-none">leave_one_out(
    metric::Type{&lt;:AccuracyMetric},
    recommender_type::Type{&lt;:Recommender},
    data::DataAccessor,
    recommender_args...
)</code></pre><p>Conduct leave-one-out cross validation (LOOCV) for a combination of recommender <code>recommender_type</code> and accuracy metric <code>metric</code>. A recommender is initialized with <code>recommender_args</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/evaluation/cross_validation.jl#L61-L70">source</a></section></article><h2 id="Rating-metrics"><a class="docs-heading-anchor" href="#Rating-metrics">Rating metrics</a><a id="Rating-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Rating-metrics" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Recommendation.RMSE" href="#Recommendation.RMSE"><code>Recommendation.RMSE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RMSE</code></pre><p>Root Mean Squared Error.</p><pre><code class="language-none">measure(
    metric::RMSE,
    truth::AbstractVector,
    pred::AbstractVector
)</code></pre><p><code>truth</code> and <code>pred</code> are expected to be the same size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/accuracy.jl#L3-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.MAE" href="#Recommendation.MAE"><code>Recommendation.MAE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MAE</code></pre><p>Mean Absolute Error.</p><pre><code class="language-none">measure(
    metric::MAE,
    truth::AbstractVector,
    pred::AbstractVector
)</code></pre><p><code>truth</code> and <code>pred</code> are expected to be the same size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/accuracy.jl#L29-L41">source</a></section></article><h2 id="Ranking-metrics"><a class="docs-heading-anchor" href="#Ranking-metrics">Ranking metrics</a><a id="Ranking-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Ranking-metrics" title="Permalink"></a></h2><p>Let a target user <span>$u \in \mathcal{U}$</span>, set of all items <span>$\mathcal{I}$</span>, ordered set of top-<span>$N$</span> recommended items <span>$I_N(u) \subset \mathcal{I}$</span>, and set of truth items <span>$\mathcal{I}^+_u$</span>.</p><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Recall" href="#Recommendation.Recall"><code>Recommendation.Recall</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Recall</code></pre><p>Recall-at-<span>$k$</span> (Recall@<span>$k$</span>) indicates coverage of truth samples as a result of top-<span>$k$</span> (<code>topk</code>) recommendation. The value is computed by the following equation:</p><p class="math-container">\[\mathrm{Recall@}k = \frac{|\mathcal{I}^+_u \cap I_N(u)|}{|\mathcal{I}^+_u|}.\]</p><p>Here, <span>$|\mathcal{I}^+_u \cap I_N(u)|$</span> is the number of <em>true positives</em>.</p><pre><code class="language-none">measure(
    metric::Recall,
    truth::AbstractVector{T},
    pred::AbstractVector{T},
    topk::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L3-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Precision" href="#Recommendation.Precision"><code>Recommendation.Precision</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Precision</code></pre><p>Precision-at-<span>$k$</span> (Precision@<span>$k$</span>) evaluates correctness of a top-<span>$k$</span> (<code>topk</code>) recommendation list <span>$I_N(u)$</span> according to the portion of true positives in the list as:</p><p class="math-container">\[\mathrm{Precision@}k = \frac{|\mathcal{I}^+_u \cap I_N(u)|}{|I_N(u)|}.\]</p><pre><code class="language-none">measure(
    metric::Precision,
    truth::AbstractVector{T},
    pred::AbstractVector{T},
    topk::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L29-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.MAP" href="#Recommendation.MAP"><code>Recommendation.MAP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MAP</code></pre><p>While the original Precision@<span>$N$</span> provides a score for a fixed-length recommendation list <span>$I_N(u)$</span>, mean average precision (MAP) computes an average of the scores over all recommendation sizes from 1 to <span>$|\mathcal{I}|$</span>. MAP is formulated with an indicator function for <span>$i_n$</span>, the <span>$n$</span>-th item of <span>$I(u)$</span>, as:</p><p class="math-container">\[\mathrm{MAP} = \frac{1}{|\mathcal{I}^+_u|} \sum_{n = 1}^{|\mathcal{I}|} \mathrm{Precision@}n \cdot  \mathbb{1}_{\mathcal{I}^+_u}(i_n).\]</p><p>It should be noticed that, MAP is not a simple mean of sum of Precision@<span>$1$</span>, Precision@<span>$2$</span>, <span>$\dots$</span>, Precision@<span>$|\mathcal{I}|$</span>, and higher-ranked true positives lead better MAP.</p><pre><code class="language-none">measure(
    metric::MAP,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L54-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.AUC" href="#Recommendation.AUC"><code>Recommendation.AUC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AUC</code></pre><p>ROC curve and area under the ROC curve (AUC) are generally used in evaluation of the classification problems, but these concepts can also be interpreted in a context of ranking problem. Basically, the AUC metric for ranking considers all possible pairs of truth and other items which are respectively denoted by <span>$i^+ \in \mathcal{I}^+_u$</span> and <span>$i^- \in \mathcal{I}^-_u$</span>, and it expects that the <span>$best&#39;&#39; recommender completely ranks$</span>i^+<span>$higher than$</span>i^-``, as follows:</p><p><img src="../assets/images/auc.png" alt="auc"/></p><p>AUC calculation keeps track the number of true positives at different rank in <span>$\mathcal{I}$</span>. At line 8, the function adds the number of true positives which were ranked higher than the current non-truth sample to the accumulated count of correct pairs. Ultimately, an AUC score is computed as portion of the correct ordered <span>$(i^+, i^-)$</span> pairs in the all possible combinations determined by <span>$|\mathcal{I}^+_u| \times |\mathcal{I}^-_u|$</span> in set notation.</p><pre><code class="language-none">measure(
    metric::AUC,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L85-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.ReciprocalRank" href="#Recommendation.ReciprocalRank"><code>Recommendation.ReciprocalRank</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ReciprocalRank</code></pre><p>If we are only interested in the first true positive, reciprocal rank (RR) could be a reasonable choice to quantitatively assess the recommendation lists. For <span>$n_{\mathrm{tp}} \in \left[ 1, |\mathcal{I}| \right]$</span>, a position of the first true positive in <span>$I(u)$</span>, RR simply returns its inverse:</p><p class="math-container">\[  \mathrm{RR} = \frac{1}{n_{\mathrm{tp}}}.\]</p><p>RR can be zero if and only if <span>$\mathcal{I}^+_u$</span> is empty.</p><pre><code class="language-none">measure(
    metric::ReciprocalRank,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L121-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.MPR" href="#Recommendation.MPR"><code>Recommendation.MPR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MPR</code></pre><p>Mean percentile rank (MPR) is a ranking metric based on <span>$r_{i} \in [0, 100]$</span>, the percentile-ranking of an item <span>$i$</span> within the sorted list of all items for a user <span>$u$</span>. It can be formulated as:</p><p class="math-container">\[\mathrm{MPR} = \frac{1}{|\mathcal{I}^+_u|} \sum_{i \in \mathcal{I}^+_u} r_{i}.\]</p><p><span>$r_{i} = 0\%$</span> is the best value that means the truth item <span>$i$</span> is ranked at the highest position in a recommendation list. On the other hand, <span>$r_{i} = 100\%$</span> is the worst case that the item <span>$i$</span> is at the lowest rank.</p><p>MPR internally considers not only top-<span>$N$</span> recommended items also all of the non-recommended items, and it accumulates the percentile ranks for all true positives unlike MRR. So, the measure is suitable to estimate users&#39; overall satisfaction for a recommender. Intuitively, <span>$\mathrm{MPR} &gt; 50\%$</span> should be worse than random ranking from a users&#39; point of view.</p><pre><code class="language-none">measure(
    metric::MPR,
    truth::AbstractVector{T},
    pred::AbstractVector{T}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L148-L164">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.NDCG" href="#Recommendation.NDCG"><code>Recommendation.NDCG</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NDCG</code></pre><p>Like MPR, normalized discounted cumulative gain (NDCG) computes a score for <span>$I(u)$</span> which places emphasis on higher-ranked true positives. In addition to being a more well-formulated measure, the difference between NDCG and MPR is that NDCG allows us to specify an expected ranking within <span>$\mathcal{I}^+_u$</span>; that is, the metric can incorporate <span>$\mathrm{rel}_n$</span>, a relevance score which suggests how likely the <span>$n$</span>-th sample is to be ranked at the top of a recommendation list, and it directly corresponds to an expected ranking of the truth samples.</p><pre><code class="language-none">measure(
    metric::NDCG,
    truth::AbstractVector{T},
    pred::AbstractVector{T},
    topk::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/ranking.jl#L180-L191">source</a></section></article><h2 id="Aggregated-metrics"><a class="docs-heading-anchor" href="#Aggregated-metrics">Aggregated metrics</a><a id="Aggregated-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Aggregated-metrics" title="Permalink"></a></h2><p>Return a single aggregated score for an array of multiple top-<span>$k$</span> recommended items. <a href="https://www.bgu.ac.il/~shanigu/Publications/EvaluationMetrics.17.pdf">Recommender Systems Handbook</a>. gives an overview of such aggregated metrics. In particular, the formulation of Gini index and Shannon Entropy can be found at Eq. (20) and (21) on page 26.</p><article class="docstring"><header><a class="docstring-binding" id="Recommendation.AggregatedDiversity" href="#Recommendation.AggregatedDiversity"><code>Recommendation.AggregatedDiversity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AggregatedDiversity</code></pre><p>The number of distinct items recommended across all suers. Larger value indicates more diverse recommendation result overall.</p><pre><code class="language-julia">measure(
    metric::AggregatedDiversity, recommendations::AbstractVector{&lt;:AbstractVector{&lt;:Integer}}
)</code></pre><p>Let <span>$U$</span> and <span>$I$</span> be a set of users and items, respectively, and <span>$L_N(u)$</span> a list of top-<span>$N$</span> recommended items for a user <span>$u$</span>. Here, an aggregated diversity can be calculated as:</p><p class="math-container">\[\left| \bigcup\limits_{u \in U} L_N(u) \right|\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/aggregated.jl#L13-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.ShannonEntropy" href="#Recommendation.ShannonEntropy"><code>Recommendation.ShannonEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ShannonEntropy</code></pre><p>If we focus more on individual items and how many users are recommended a particular item, the diversity of <code>topk</code> recommender can be defined by <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon Entropy</a>:</p><p class="math-container">\[-\sum_{j = 1}^{|I|} \left( \frac{\left|\{u \mid u \in U \wedge i_j \in L_N(u) \}\right|}{N |U|} \ln \left( \frac{\left|\{u \mid u \in U \wedge i_j \in L_N(u) \}\right|}{N |U|}  \right) \right)\]</p><p>where <span>$i_j$</span> denotes <span>$j$</span>-th item in the available item set <span>$I$</span>.</p><pre><code class="language-julia">measure(
    metric::ShannonEntropy, recommendations::AbstractVector{&lt;:AbstractVector{&lt;:Integer}};
    topk::Integer
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/aggregated.jl#L45-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.GiniIndex" href="#Recommendation.GiniIndex"><code>Recommendation.GiniIndex</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GiniIndex</code></pre><p><a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini Index</a>, which is normally used to measure a degree of inequality in a distribution of income, can be applied to assess diversity in the context of <code>topk</code> recommendation:</p><p class="math-container">\[\frac{1}{|I| - 1} \sum_{j = 1}^{|I|} \left( (2j - |I| - 1) \cdot \frac{\left|\{u \mid u \in U \wedge i_j \in L_N(u) \}\right|}{N |U|} \right)\]</p><pre><code class="language-julia">measure(
    metric::GiniIndex, recommendations::AbstractVector{&lt;:AbstractVector{&lt;:Integer}};
    topk::Integer
)</code></pre><p>The index is 0 when all items are equally chosen in terms of the number of recommended users.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/aggregated.jl#L77-L94">source</a></section></article><h2 id="Intra-list-metrics"><a class="docs-heading-anchor" href="#Intra-list-metrics">Intra-list metrics</a><a id="Intra-list-metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Intra-list-metrics" title="Permalink"></a></h2><p>Given a list of recommended items (for a single user), intra-list metrics quantifies the quality of the recommendation list from a non-accuracy perspective. <a href="https://dl.acm.org/doi/10.1016/j.knosys.2016.08.014">A Survey of Serendipity in Recommender Systems</a> highlights the foundation of these metrics.</p><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Coverage" href="#Recommendation.Coverage"><code>Recommendation.Coverage</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Coverage</code></pre><p>Catalog coverage is a ratio of recommended items among <code>catalog</code>, which represents a set of all available items.</p><pre><code class="language-julia">measure(
    metric::Coverage, recommendations::Union{AbstractSet, AbstractVector};
    catalog::Union{AbstractSet, AbstractVector}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/intra_list.jl#L13-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Novelty" href="#Recommendation.Novelty"><code>Recommendation.Novelty</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Novelty</code></pre><p>The number of recommended items that have not been observed yet i.e., not in <code>observed</code>.</p><pre><code class="language-julia">measure(
    metric::Novelty, recommendations::Union{AbstractSet, AbstractVector};
    observed::Union{AbstractSet, AbstractVector}
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/intra_list.jl#L30-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.IntraListSimilarity" href="#Recommendation.IntraListSimilarity"><code>Recommendation.IntraListSimilarity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">IntraListSimilarity</code></pre><p>Sum of similarities between every pairs of recommended items. Larger value represents less diversity.</p><ul><li>Reference: <a href="http://files.grouplens.org/papers/ziegler-www05.pdf">Improving Recommendation Lists Through Topic Diversification</a></li></ul><pre><code class="language-julia">measure(
    metric::IntraListSimilarity, recommendations::Union{AbstractSet, AbstractVector};
    sims::AbstractMatrix
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/intra_list.jl#L48-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Recommendation.Serendipity" href="#Recommendation.Serendipity"><code>Recommendation.Serendipity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Serendipity</code></pre><p>Return a sum of relevance-unexpectedness multiplications for all recommended items.</p><pre><code class="language-julia">measure(
    metric::Serendipity, recommendations::Union{AbstractSet, AbstractVector};
    relevance::AbstractVector, unexpectedness::AbstractVector
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/takuti/Recommendation.jl/blob/738888db9a4772d734b86feed9ca0e089cefa764/src/metrics/intra_list.jl#L67-L78">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../content_based_filtering/">« Content-Based Filtering</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 20 November 2022 21:11">Sunday 20 November 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
