This paper introduced \texttt{Recommendation.jl}, an open-source package for building recommender systems in the Julia programming language. First, by reviewing each of the core features of practical recommender pipelines, data model (\sect{data}), recommender interface and algorithms (\sect{algorithm}), and evaluation methods (\sect{evaluation}), we observed how diverse recommender's interests can be; the applications must be able to address both explicit and implicit representation of user feedback, hybridize rule-based and machine learning-based algorithms, and assess the outcomes from wide-ranging perspectives in terms of not only accuracy but diversity, coverage, novelty, and serendipity. Thus, Julia's extensible and mathematical operation-friendly APIs come in handy for working with the unique characteristics we demonstrated by their formulation and corresponding code snippet throughout the paper.

Moreover, we conducted a benchmark with multiple recommender-metric pairs provided by \texttt{Recommendation.jl} and confirmed there are no one-size-fits-all approaches to making ``good'' recommendations. On the one hand, we can maximize prediction accuracy by training a sophisticated model-based recommender with an optimal set of hyperparameters. However, at the same time, the best prediction accuracy does not always yield the most diverse recommendation, which might eventually hinder recommenders from acknowledging fairness implications. The observations tell us that one of the most important requirements for recommender frameworks is to make a wide variety of options available for developers while leaving enough space for customization, which \texttt{Recommendation.jl} has tried to incorporate by design.

Finally, there are numerous possible directions to improve the package as we learned from the other open-source solutions in \sect{introduction}. For instance, the availability of state-of-the-art recommendation algorithms makes a framework more promising in a competitive environment in the industry, where Python-based machine learning packages play a dominant role. Meanwhile, since computational efficiency is a key criterion that directly leads to a developer's productivity, the use of acceleration techniques such as distributed multiprocessing and GPU programming would be a mandatory step to undergo. Last but not least, easing to run an end-to-end recommendation pipeline iteratively is a foundational challenge so we can bridge a gap between an offline and online setup. In particular, evaluation phases pose a crucial challenge in reproducibility as mentioned in \sect{evaluation}.
